{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afd6fe9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b036b7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install deep-sort-realtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe7cbd",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2bbc7",
   "metadata": {},
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29de49c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892abb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51da8ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ultralytics.models.yolo.model.YOLO at 0x28098f0a250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import pyvirtualcam\n",
    "import math\n",
    "import numpy as np\n",
    "from KalmanFilter import KalmanFilter\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../pyNDI/ndi')\n",
    "#pyNDI Import\n",
    "import finder\n",
    "import receiver\n",
    "import lib\n",
    "\n",
    "find = finder.create_ndi_finder()\n",
    "NDIsources = find.get_sources()\n",
    "recieveSource = NDIsources[0]\n",
    "reciever = receiver.create_receiver(recieveSource)\n",
    "\n",
    "# define some constants\n",
    "\n",
    "GREEN = (0, 255, 0)\n",
    "w_fhd = int(1920/2)\n",
    "h_fhd = int(1080/2)\n",
    "w_hd=int(1280/2)\n",
    "h_hd=int(720/2)\n",
    "\n",
    "KF = KalmanFilter(0.1, 1, 5, 1, 20, 20)\n",
    "# Video from BGR to RGB\n",
    "fmt = pyvirtualcam.PixelFormat.BGR\n",
    "# load the pre-trained YOLOv8n model\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4934e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.4\n",
    "f_total = 0\n",
    "f_segm = 0\n",
    "debugging = False\n",
    "testing = False\n",
    "movement_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab278e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds(x, y, relative_w, relative_h, real_w, real_h):\n",
    "    xmin = 0\n",
    "    xmax = 0\n",
    "    ymin = 0\n",
    "    ymax = 0\n",
    "    if x < relative_w:\n",
    "        xmin = 0\n",
    "        xmax = relative_w*2\n",
    "    elif x > (real_w-relative_w):\n",
    "        xmin = real_w-(relative_w*2)\n",
    "        xmax = real_w\n",
    "    else:\n",
    "        xmin = int(x-relative_w)\n",
    "        xmax = int(x+relative_w)\n",
    "    \n",
    "    if y < relative_h:\n",
    "        ymin = 0\n",
    "        ymax = relative_h*2\n",
    "    elif y > (real_h-relative_h):\n",
    "        ymin = real_h-(relative_h*2)\n",
    "        ymax = real_h\n",
    "    else:\n",
    "        ymin = int(y-relative_h)\n",
    "        ymax = int(y+relative_h)\n",
    "    return xmin, xmax, ymin, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e20840e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\EL ENCUENTRO\\Documents\\Tracking\\image-tracking\\src\\Tracking.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EL%20ENCUENTRO/Documents/Tracking/image-tracking/src/Tracking.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EL%20ENCUENTRO/Documents/Tracking/image-tracking/src/Tracking.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Captura de video\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/EL%20ENCUENTRO/Documents/Tracking/image-tracking/src/Tracking.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m frame \u001b[39m=\u001b[39m reciever\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EL%20ENCUENTRO/Documents/Tracking/image-tracking/src/Tracking.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_RGBA2RGB)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EL%20ENCUENTRO/Documents/Tracking/image-tracking/src/Tracking.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Se corre YOLO\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EL ENCUENTRO\\Documents\\Tracking\\image-tracking\\src\\../pyNDI/ndi\\receiver.py:61\u001b[0m, in \u001b[0;36mNDIReceiver.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m# Loop until we get data\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     res_val \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mNDIlib_recv_capture_v2(pNDI_recv, video_frame, ffi\u001b[39m.\u001b[39;49mNULL, ffi\u001b[39m.\u001b[39;49mNULL, \u001b[39m1000\u001b[39;49m)\n\u001b[0;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m res_val \u001b[39m==\u001b[39m FrameType\u001b[39m.\u001b[39mtype_video:\n\u001b[0;32m     64\u001b[0m         width \u001b[39m=\u001b[39m video_frame\u001b[39m.\u001b[39mxres\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "centroidX = w_fhd\n",
    "centroidY = h_fhd\n",
    "#keyPoints = [0, 5, 6]\n",
    "(x1, y1) = KF.update([w_fhd, h_fhd])\n",
    "with pyvirtualcam.Camera(width=1920, height=1080, fps=60, fmt=fmt) as cam:\n",
    "    while True:\n",
    "        # Fotograma vacio\n",
    "        frameProcessed = np.zeros((cam.height, cam.width, 3), np.uint8)\n",
    "        \n",
    "        # Inicio de tiempo para medir\n",
    "        start = datetime.datetime.now()\n",
    "    \n",
    "        # Captura de video\n",
    "        frame = reciever.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "        # Se corre YOLO\n",
    "        detections = model(frame, \n",
    "                           classes=0, \n",
    "                           conf=CONFIDENCE_THRESHOLD, \n",
    "                           verbose=False)[0].keypoints.data.tolist()\n",
    "        \n",
    "        temp_centroidX = 0\n",
    "        temp_centroidY = 0\n",
    "        total_length = 0\n",
    "        \n",
    "        for data in detections:\n",
    "            if len(data)>0:\n",
    "                for i, info in enumerate(data):\n",
    "                    temp_centroidX = temp_centroidX + data[i][0]\n",
    "                    temp_centroidY = temp_centroidY + data[i][1]\n",
    "                    if debugging:\n",
    "                        cv2.circle(frame, (int(data[i][0]), int(data[i][1])), radius=5, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "                total_length = total_length + len(data)\n",
    "\n",
    "        if total_length > 0:\n",
    "            temp_centroidX = temp_centroidX/total_length\n",
    "            temp_centroidY = temp_centroidY/total_length - 150\n",
    "            temp_distanceCenter = np.sqrt((temp_centroidX-centroidX)**2+(temp_centroidY-centroidY)**2)\n",
    "\n",
    "\n",
    "            if temp_distanceCenter > movement_threshold:\n",
    "                centroidX = int(temp_centroidX)\n",
    "                centroidY = int(temp_centroidY)\n",
    "\n",
    "\n",
    "        if len(detections) > 0:   \n",
    "            focus = np.asarray(KF.predict()[0]).reshape(-1)\n",
    "            (x1, y1) = KF.update([centroidX, centroidY])\n",
    "\n",
    "        if debugging:\n",
    "            cv2.circle(frame, (centroidX, centroidY), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            cv2.circle(frame, (int(focus[0]), int(focus[1])), radius=5, color=(0, 255, 0), thickness=-1)\n",
    "            \n",
    "        # Calculo del enfoque \n",
    "        #xmin_p, xmax_p, ymin_p, ymax_p = bounds(int(focus[0]), int(focus[1]), w_hd, h_hd, 1920, 1080)\n",
    "        xmin_p, xmax_p, ymin_p, ymax_p = bounds(int(focus[0]), h_hd+200, w_hd, h_hd, 1920, 1080)\n",
    "        frame = frame[ymin_p:ymax_p, xmin_p:xmax_p]\n",
    "\n",
    "        # Aumentado del tama√±o\n",
    "        frame = cv2.resize(frame, (1920, 1080), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        if debugging:           \n",
    "            # end time to compute the fps\n",
    "            end = datetime.datetime.now()\n",
    "            # show the time it took to process 1 frame\n",
    "            total = (end - start).total_seconds()\n",
    "            #print(f\"Time to process 1 frame: {total * 1000:.0f} milliseconds\")\n",
    "\n",
    "            # calculate the frame per second and draw it on the frame\n",
    "            fps = f\"FPS: {1 / total:.2f}\"\n",
    "            cv2.putText(frame, fps, (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 8)\n",
    "\n",
    "        cam.send(frame)\n",
    "        cam.sleep_until_next_frame()\n",
    "\n",
    "    video_cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a1794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-tracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "822627c4390a231fd1f8aba98ff5d82ebd42a0deecf7a6daff8b5a9f5d3990c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
